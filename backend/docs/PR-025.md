# PR-025 — Short-TTL Caching for Overview Endpoint

## Goal

Introduce short-TTL Redis caching for the `/api/overview` endpoint to significantly reduce repeated computation and external candle fetches under load.

This improves latency, scalability, and API stability without sacrificing meaningful freshness.

---

## Motivation

`/api/overview` performs:

* Ranking computation
* Multiple candle fetches (N symbols)
* Sparkline aggregation

Under traffic, this becomes expensive:

* Repeated candle reads
* Repeated scoring
* Increased external API pressure

However, overview data does not require second-level precision.

A short TTL (e.g., 30–60 seconds) provides strong performance gains while keeping the UI fresh.

---

## Scope

### Included

* Redis decorator for Overview use case
* Configurable TTL
* Cache key based on timeframe + limit
* Graceful Redis failure fallback

### Excluded

* Per-symbol partial caching
* Background refresh worker
* Distributed cache invalidation

---

## Architecture

Create decorator in:

```
infrastructure/overview/redis_cached_overview.go
```

Wraps:

```
GetOverview use case
```

---

## Decorator Design

```go
type RedisCachedOverview struct {
    next  OverviewUseCase
    redis RedisClient
    ttl   time.Duration
    keyPrefix string
}
```

Where:

* `next` → GetOverview use case
* `redis` → injected Redis abstraction
* `ttl` → e.g., 30s
* `keyPrefix` → "overview"

---

## Cache Key Strategy

Key must include request parameters.

Example:

```
overview:1h:10
```

Format:

```
{prefix}:{timeframe}:{limit}
```

This prevents collisions between different overview queries.

---

## Behavior

### On `Execute(ctx, timeframe, limit)`:

1. Generate cache key
2. Attempt Redis GET
3. If hit:

   * Deserialize OverviewResponse
   * Return cached value
4. If miss:

   * Call underlying use case
   * Serialize response
   * Store with TTL
   * Return response

---

## TTL Strategy

Recommended default: **45 seconds**

Reasoning:

* Overview is visual scanning tool
* Minute-level freshness is sufficient
* Dramatically reduces redundant candle fetches

TTL must be configurable via environment:

```
OVERVIEW_CACHE_TTL_SECONDS=45
```

Clamp rules at startup:

* Min: 5 seconds
* Max: 300 seconds

---

## Serialization

Serialize entire `OverviewResponse` as JSON.

This avoids recomputation and preserves ordering.

---

## Error Handling

* Redis GET failure → log + fallback
* Redis SET failure → ignore, still return fresh result
* JSON unmarshal error → treat as cache miss
* Underlying use case error → return error (do not cache)

Redis must never break endpoint.

---

## Performance Impact

Before:

* Ranking computation per request
* N candle fetches per request

After:

* Full computation once per TTL window
* Subsequent requests served in <5ms

This provides exponential scalability improvement.

---

## Acceptance Criteria

* Repeated identical overview calls within TTL hit Redis
* Latency significantly reduced after first call
* Different timeframe/limit combinations use different keys
* Redis failure does not break endpoint
* Unit tests cover hit and miss paths

---

## Tests

Location:

```
test/infrastructure/overview/
```

Required test cases:

1. ReturnsCachedOverviewWhenPresent
2. CallsNextOnCacheMiss
3. StoresValueWithTTL
4. DifferentKeysForDifferentParams
5. FallsBackOnRedisFailure
6. DoesNotCacheOnNextError

---

## Future Enhancements

* Background refresh before expiration
* Cache stampede protection (singleflight)
* Metrics: hit/miss ratio
* Per-timeframe TTL tuning

---

## Product Impact

This PR transforms `/api/overview` into a highly scalable endpoint suitable for:

* Public dashboards
* High-traffic scanning
* Frequent polling UIs

Together with PR-021 and PR-022, the system becomes production-grade and rate-limit safe.
